# -*- coding: utf-8 -*-
"""oasis_implementacion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19urX-6EIpcSpWCmCxwvknuUoCIUWXyCA

**DATASET OASIS**

________________________________________________________________________________
UNIVERSIDAD AUTÓNOMA DE CHIHUAHUA

FACULTAD DE INGENIERÍA

MAESTRÍA EN INGENIERÍA EN COMPUTACIÓN

MATERIA: MACHINE LEARNING

DOCENTE: OLANDA PRIETO ORDAZ

PROYECTO FINAL DE LA MATERIA

ALUMNO: HEBER ABRAHAM ZAPATA ROBLES

MATRÍCULA: 329454
________________________________________________________________________________
"""
# importar librerías
import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, KFold, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score
import joblib
# obtener datos
ruta = 'modelo/oasis_longitudinal_demographics-8d83e569fa2e2d30.xlsx'
oasis = pd.read_excel (ruta)
oasis_copy = oasis.copy()

# PREPARAR DATOS

# funcion para preparar los datos
def preparar_datos(oasis):
    # eliminar columnas innecesarias
    columnas_innecesarias = ['Subject ID', 'MRI ID', 'Visit', 'Hand']
    oasis_preparado = oasis.drop(columns=columnas_innecesarias)
    # Converted a Demented en la columna Group
    oasis_preparado['Group'] = oasis_preparado['Group'].replace('Converted', 'Demented')
    # Mapear las clases Nondemented a 0 y Demented a 1
    mapeo = {'Nondemented': 0, 'Demented': 1}
    oasis_preparado['Group'] = oasis_preparado['Group'].map(mapeo)
    return oasis_preparado
# funcion para preparar el dataset de 150 pacientes
def oasis_150 (oasis):
    # seleccionar la visita 1
    oasis_select = oasis.loc[oasis['Visit'] == 1].reset_index(drop=True)
    return oasis_select

# DATASET 150

# preparar el dataset
oasis_new = oasis_150 (oasis)
oasis_new = preparar_datos(oasis_new)
# columnas categóricas y numéricas
categorical_features = ['M/F']
numeric_features = oasis_new.drop(columns=['M/F', 'Group']).columns
# preprocesador para transformar las columnas categóricas y numéricas
preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='mean')),
            ('scaler', StandardScaler())
        ]), numeric_features),
        ('cat', Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(drop='first'))
        ]), categorical_features)
    ])
# aplicar transformaciones
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor)
])
# dividir target y características
X_150 = oasis_new.drop(columns='Group')
y_150 = oasis_new['Group']
# separación del dataset, manteniendo la proporción del target
X_train, X_test, y_150_train, y_150_test = train_test_split(X_150, y_150, test_size=0.2, random_state=42, stratify=y_150)
# aplicar el pipeline al conjunto de entrenamiento
X_150_train = pipeline.fit_transform(X_train)
X_150_test = pipeline.transform(X_test)
# verificar la separacion
# set entrenamiento
train_class_distribution = pd.Series(y_150_train).value_counts(normalize=True)
print("Distribución de clases en el conjunto de entrenamiento:")
print(train_class_distribution)
# set prueba
test_class_distribution = pd.Series(y_150_test).value_counts(normalize=True)
print("\nDistribución de clases en el conjunto de prueba:")
print(test_class_distribution)

# DATASET 373

# preparar el dataset
oasis_copy = preparar_datos (oasis_copy)
# columnas categóricas y numéricas
categorical_features = ['M/F']
numeric_features = oasis_copy.drop(columns=['M/F', 'Group']).columns
# preprocesador para transformar las columnas categóricas y numéricas
preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='mean')),
            ('scaler', StandardScaler())
        ]), numeric_features),
        ('cat', Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(drop='first'))
        ]), categorical_features)
    ])
# aplicar transformaciones
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor)
])
# dividir target y características
X_373 = oasis_copy.drop(columns='Group')
y_373 = oasis_copy['Group']
# separación del dataset, manteniendo la proporción del target
X_train, X_test, y_373_train, y_373_test = train_test_split(X_373, y_373, test_size=0.2, random_state=42, stratify=y_373)
# aplicar el pipeline al conjunto de entrenamiento
X_373_train = pipeline.fit_transform(X_train)
joblib.dump(pipeline, 'pipeline_373.pkl')
X_373_test = pipeline.transform(X_test)
# verificar la separacion
# set entrenamiento
train_class_distribution = pd.Series(y_373_train).value_counts(normalize=True)
print("Distribución de clases en el conjunto de entrenamiento:")
print(train_class_distribution)
# set prueba
test_class_distribution = pd.Series(y_373_test).value_counts(normalize=True)
print("\nDistribución de clases en el conjunto de prueba:")
print(test_class_distribution)

# logistic_150 regression con KFold y GridSearch

# preprocesar datos
X_150_processed = pipeline.fit_transform(X_150)
# parámetros para GridSearch
param_grid = {
    'C': [0.1, 1, 10],  # diferentes valores de regularización
    'solver': ['liblinear', 'saga']  # solvers a probar
}
# modelo de regresión logística
modelo_150 = LogisticRegression()
# KFold con 5 particiones, shuffle para mezclar los datos antes de dividirlos
kf_150 = KFold(n_splits=5, shuffle=True, random_state=42)
# GridSearchCV con f1 ponderado como métrica y validación cruzada con KFold
grid_search = GridSearchCV(estimator=modelo_150, param_grid=param_grid,
                           scoring='f1_weighted', cv=kf_150)
# ajustar el GridSearch con los datos preprocesados
grid_search.fit(X_150_processed, y_150)
# mejor modelo encontrado por GridSearch
best_model = grid_search.best_estimator_
# mejores parámetros
best_params = grid_search.best_params_
# mostrar mejores parámetros
print(f'Mejores parámetros: {best_params}')
# listas para almacenar métricas de cada fold
accuracy_list_150 = []
precision_list_150 = []
recall_list_150 = []
f1_list_150 = []
# aplicar KFold para validación cruzada
for fold, (train_index, test_index) in enumerate(kf_150.split(X_150_processed), 1):
    # dividir los datos en conjuntos de entrenamiento y prueba usando los índices de KFold
    X_train_150, X_test_150 = X_150_processed[train_index], X_150_processed[test_index]
    y_train_150, y_test_150 = y_150[train_index], y_150[test_index]
    # entrenar el mejor modelo con los datos de entrenamiento
    best_model.fit(X_train_150, y_train_150)
    # predecir en el conjunto de prueba
    y_pred_150 = best_model.predict(X_test_150)
    # calcular las métricas
    accuracy_150 = accuracy_score(y_test_150, y_pred_150)
    precision_150 = precision_score(y_test_150, y_pred_150, average='weighted')
    recall_150 = recall_score(y_test_150, y_pred_150, average='weighted')
    f1_150 = f1_score(y_test_150, y_pred_150, average='weighted')
    # guardar las métricas en sus respectivas listas
    accuracy_list_150.append(accuracy_150)
    precision_list_150.append(precision_150)
    recall_list_150.append(recall_150)
    f1_list_150.append(f1_150)
    # resultados para cada fold
    print(f'\nFold {fold}:')
    print(f'Accuracy: {accuracy_150}')
    print(f'Precision: {precision_150}')
    print(f'Recall: {recall_150}')
    print(f'F1 Score: {f1_150}')
    print(f'Matriz de confusión:\n{confusion_matrix(y_test_150, y_pred_150)}')
    print(f'Reporte de clasificación:\n{classification_report(y_test_150, y_pred_150)}')
# mostrar el promedio de todas las métricas después de 5 folds
print(f'\nPromedio de accuracy: {np.mean(accuracy_list_150)}')
print(f'Promedio de precision: {np.mean(precision_list_150)}')
print(f'Promedio de recall: {np.mean(recall_list_150)}')
print(f'Promedio de F1 Score: {np.mean(f1_list_150)}')
# guardar el mejor modelo en un archivo .pkl
joblib.dump(best_model, 'best_lr_model_150.pkl')

# logistic_373 regression con KFold y Gridsearch

# parámetros de gridsearch
param_grid = {
    'C': [0.1, 1, 10],  # diferentes valores de regularización
    'solver': ['liblinear', 'saga']  # solvers a probar
}
# modelo de regresión logística
modelo_373 = LogisticRegression()
# KFold con 5 particiones, shuffle para mezclar los datos antes de dividirlos
kf_373 = KFold(n_splits=5, shuffle=True, random_state=42)
# GridSearchCV con f1 ponderado como métrica y validación cruzada con KFold
grid_search = GridSearchCV(estimator=modelo_373, param_grid=param_grid,
                           scoring='f1_weighted', cv=kf_373)
# entrenar el GridSearch con los datos preprocesados
grid_search.fit(X_373_train, y_373_train)
# mejor modelo encontrado por GridSearch
best_model = grid_search.best_estimator_
# mejores parámetros
best_params = grid_search.best_params_
# mostrar los mejores parámetros
print(f'Mejores parámetros: {best_params}')
# listas para almacenar métricas de cada fold
accuracy_list_373 = []
precision_list_373 = []
recall_list_373 = []
f1_list_373 = []
# aplicar KFold para validación cruzada
for fold, (train_index, test_index) in enumerate(kf_373.split(X_373_train), 1):
    # dividir los datos en conjuntos de entrenamiento y prueba usando los índices de KFold
    # reindexar para evitar errores de índices
    X_train_373, X_test_373 = X_373_train[train_index], X_373_train[test_index]
    y_train_373, y_test_373 = y_373_train.iloc[train_index], y_373_train.iloc[test_index]
    # entrenar el mejor modelo con los datos de entrenamiento
    best_model.fit(X_train_373, y_train_373)
    # predecir en el conjunto de prueba
    y_pred_373 = best_model.predict(X_test_373)
    # calcular las métricas
    accuracy_373 = accuracy_score(y_test_373, y_pred_373)
    precision_373 = precision_score(y_test_373, y_pred_373, average='weighted')
    recall_373 = recall_score(y_test_373, y_pred_373, average='weighted')
    f1_373 = f1_score(y_test_373, y_pred_373, average='weighted')
    # guardar las métricas en sus respectivas listas
    accuracy_list_373.append(accuracy_373)
    precision_list_373.append(precision_373)
    recall_list_373.append(recall_373)
    f1_list_373.append(f1_373)
    # resultados para cada fold
    print(f'\nFold {fold}:')
    print(f'Accuracy: {accuracy_373}')
    print(f'Precision: {precision_373}')
    print(f'Recall: {recall_373}')
    print(f'F1 Score: {f1_373}')
    print(f'Matriz de confusión:\n{confusion_matrix(y_test_373, y_pred_373)}')
    print(f'Reporte de clasificación:\n{classification_report(y_test_373, y_pred_373)}')
# mostrar el promedio de todas las métricas después de 5 folds
print(f'\nPromedio de accuracy: {np.mean(accuracy_list_373)}')
print(f'Promedio de precision: {np.mean(precision_list_373)}')
print(f'Promedio de recall: {np.mean(recall_list_373)}')
print(f'Promedio de F1 Score: {np.mean(f1_list_373)}')
# guardar el mejor modelo en un archivo .pkl
joblib.dump(best_model, 'best_lr_model_373.pkl')

# svm_lineal_150 con KFold y Gridsearch

# parámetros de gridsearch
param_grid = {
    'C': [0.1, 1, 10],  # diferentes valores del parámetro de regularización
    'kernel': ['linear']  # se usa el kernel lineal
}
# modelo de SVM
modelo_150 = SVC()
# KFold con 5 particiones, shuffle para mezclar los datos antes de dividir
kf_150 = KFold(n_splits=5, shuffle=True, random_state=42)
# GridSearchCV con f1 ponderado como métrica y validación cruzada con KFold
grid_search = GridSearchCV(estimator=modelo_150, param_grid=param_grid,
                           scoring='f1_weighted', cv=kf_150)
# entrenar el GridSearchCV con los datos de entrenamiento
grid_search.fit(X_150_train, y_150_train)
# mejor modelo encontrado por GridSearch
best_model = grid_search.best_estimator_
# mejores parámetros
best_params = grid_search.best_params_
# mostrar los mejores parámetros
print(f'Mejores parámetros: {best_params}')
# listas para almacenar métricas de cada fold
accuracy_list_150 = []
precision_list_150 = []
recall_list_150 = []
f1_list_150 = []
# aplicar KFold para validación cruzada
for fold, (train_index, test_index) in enumerate(kf_150.split(X_150_train), 1):
    # dividir los datos en conjuntos de entrenamiento y prueba usando los índices de KFold
    X_train_150, X_test_150 = X_150_train[train_index], X_150_train[test_index]
    y_train_150, y_test_150 = y_150_train.iloc[train_index], y_150_train.iloc[test_index]
    # entrenar el mejor modelo con los datos de entrenamiento
    best_model.fit(X_train_150, y_train_150)
    # predecir en el conjunto de prueba
    y_pred_150 = best_model.predict(X_test_150)
    # calcular las métricas
    accuracy_150 = accuracy_score(y_test_150, y_pred_150)
    precision_150 = precision_score(y_test_150, y_pred_150, average='weighted')
    recall_150 = recall_score(y_test_150, y_pred_150, average='weighted')
    f1_150 = f1_score(y_test_150, y_pred_150, average='weighted')
    # guardar las métricas en sus respectivas listas
    accuracy_list_150.append(accuracy_150)
    precision_list_150.append(precision_150)
    recall_list_150.append(recall_150)
    f1_list_150.append(f1_150)
    # resultados para cada fold
    print(f'\nFold {fold}:')
    print(f'Accuracy: {accuracy_150}')
    print(f'Precision: {precision_150}')
    print(f'Recall: {recall_150}')
    print(f'F1 Score: {f1_150}')
    print(f'Matriz de confusión:\n{confusion_matrix(y_test_150, y_pred_150)}')
    print(f'Reporte de clasificación:\n{classification_report(y_test_150, y_pred_150)}')
# mostrar el promedio de todas las métricas después de 5 folds
print(f'\nPromedio de accuracy: {np.mean(accuracy_list_150)}')
print(f'Promedio de precision: {np.mean(precision_list_150)}')
print(f'Promedio de recall: {np.mean(recall_list_150)}')
print(f'Promedio de F1 Score: {np.mean(f1_list_150)}')
# guardar el mejor modelo en un archivo .pkl
joblib.dump(best_model, 'best_svm_model_150.pkl')

# svm_lineal_373 con KFold y Gridsearch

# parámetros de GridSearch
param_grid = {
    'C': [0.1, 1, 10],  # valores de regularización
    'kernel': ['linear']  # kernel lineal
}
# modelo de SVM
modelo_373 = SVC()
# configuración de KFold con 5 particiones, y shuffle=True para mezclar los datos
kf_373 = KFold(n_splits=5, shuffle=True, random_state=42)
# GridSearchCV con f1 ponderado como métrica y KFold para validación cruzada
grid_search = GridSearchCV(estimator=modelo_373, param_grid=param_grid,
                           scoring='f1_weighted', cv=kf_373)
# entrenar el GridSearchCV con los datos de entrenamiento
grid_search.fit(X_373_train, y_373_train)
# mejor modelo encontrado por GridSearchCV
best_model = grid_search.best_estimator_
# mejor conjunto de hiperparámetros encontrados
best_params = grid_search.best_params_
# imprimir los mejores parámetros
print(f'Mejores parámetros: {best_params}')
# listas para almacenar las métricas de cada fold
accuracy_list_373 = []
precision_list_373 = []
recall_list_373 = []
f1_list_373 = []
# aplicar KFold para realizar validación cruzada
for fold, (train_index, test_index) in enumerate(kf_373.split(X_373_train), 1):
    # dividir los datos en conjuntos de entrenamiento y prueba usando los índices de KFold
    X_train_373, X_test_373 = X_373_train[train_index], X_373_train[test_index]
    y_train_373, y_test_373 = y_373_train.iloc[train_index], y_373_train.iloc[test_index]
    # entrenar el mejor modelo con los datos de entrenamiento
    best_model.fit(X_train_373, y_train_373)
    # predecir en el conjunto de prueba
    y_pred_373 = best_model.predict(X_test_373)
    # calcular las métricas
    accuracy_373 = accuracy_score(y_test_373, y_pred_373)
    precision_373 = precision_score(y_test_373, y_pred_373, average='weighted')
    recall_373 = recall_score(y_test_373, y_pred_373, average='weighted')
    f1_373 = f1_score(y_test_373, y_pred_373, average='weighted')
    # guardar las métricas en sus respectivas listas
    accuracy_list_373.append(accuracy_373)
    precision_list_373.append(precision_373)
    recall_list_373.append(recall_373)
    f1_list_373.append(f1_373)
    # resultados para cada fold
    print(f'\nFold {fold}:')
    print(f'Accuracy: {accuracy_373}')
    print(f'Precision: {precision_373}')
    print(f'Recall: {recall_373}')
    print(f'F1 Score: {f1_373}')
    print(f'Matriz de confusión:\n{confusion_matrix(y_test_373, y_pred_373)}')
    print(f'Reporte de clasificación:\n{classification_report(y_test_373, y_pred_373)}')
# mostrar el promedio de todas las métricas después de 5 folds
print(f'\nPromedio de accuracy: {np.mean(accuracy_list_373)}')
print(f'Promedio de precision: {np.mean(precision_list_373)}')
print(f'Promedio de recall: {np.mean(recall_list_373)}')
print(f'Promedio de F1 Score: {np.mean(f1_list_373)}')
# guardar el mejor modelo en un archivo .pkl
joblib.dump(best_model, 'best_svm_model_373.pkl')